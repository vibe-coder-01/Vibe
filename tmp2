import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.svm import OneClassSVM
from pandas.api.types import is_bool_dtype, is_object_dtype
import ruptures as rpt
from statsmodels.tsa.stattools import adfuller
from statsmodels.stats.diagnostic import het_white
import warnings
warnings.filterwarnings('ignore')

# For Bayesian changepoint detection
try:
    import pymc3 as pm
    import theano.tensor as tt
    BAYESIAN_AVAILABLE = True
except ImportError:
    try:
        # Alternative lightweight Bayesian implementation
        from scipy.special import gammaln, loggamma
        BAYESIAN_AVAILABLE = True
    except ImportError:
        BAYESIAN_AVAILABLE = False

# For additional changepoint detection packages
try:
    import Rbeast as rb
    RBEAST_AVAILABLE = True
except ImportError:
    RBEAST_AVAILABLE = False

try:
    import changepoint as cpt
    CPT_AVAILABLE = True
except ImportError:
    CPT_AVAILABLE = False

class ComprehensiveShiftDetection:
    """
    Comprehensive shift detection suite combining CUSUM with advanced methods
    """
    
    def __init__(self, df, target_vars, date_col='GMT Prod Yield Date', product_col='Merge'):
        self.df = df.copy()
        self.target_vars = target_vars
        self.date_col = date_col
        self.product_col = product_col
        self.shift_results = {}
        
    def ts_assign_changepoints(self, df, column, package="rb", model=None, verbose=False, **package_kwargs):
        """
        Detects changepoints of a selected column in dataframe and assign a new boolean column "_changepoint".
        Integrated from external function with error handling.
        """
        results = {'shifts': [], 'method': f'ts_assign_{package}', 'details': {}}
        
        try:
            # converting column to numeric
            if is_bool_dtype(df[column]):
                convert_kwargs = {column: df[column].mask(df[column].isna(), True).astype(int).mask(df[column].isna(), np.nan)}
            elif is_object_dtype(df[column]):
                convert_kwargs = {column: df[column].astype("category").cat.codes.to_frame().apply(lambda x: x/max(x), axis=0).iloc[:,0].mask(df[column].isna(), np.nan)}
            else:
                convert_kwargs = {}
            df_ = df.assign(**convert_kwargs)

            if package.lower() in ["rb", "rbeast"]:
                if not RBEAST_AVAILABLE:
                    results['error'] = 'Rbeast package not available'
                    return results
                
                if "season" not in package_kwargs.keys():
                    package_kwargs.update({"season": "none"})
                if "quiet" not in package_kwargs.keys():
                    package_kwargs.update({"quiet": 1})

                rb_fitted = rb.beast(df_[column], verbose=verbose, **package_kwargs)
                if verbose:
                    rb.plot(rb_fitted)

                ts_changepoints = rb_fitted.trend.cp  # list of iloc with change

            elif package.lower() == "rpt":
                if model is None:
                    model = "rpt.Pelt(model = 'l2')"
                elif model[-1] != ")":
                    model = model + "()"

                model = eval(model)
                model.fit(df_[column].bfill().values)

                if not (("pen" in package_kwargs.keys()) or 
                       ("n_bkps" in package_kwargs.keys()) or
                       ("epsilon" in package_kwargs.keys())):
                    package_kwargs.update({"pen": df_.shape[0]*0.005})

                ts_changepoints = model.predict(**package_kwargs)
                ts_changepoints = [i for i in ts_changepoints if i < df_.shape[0]]

                if verbose:
                    rpt.display(df_[column].values, ts_changepoints)
                    plt.show()

            elif package.lower() == "ocsvm":
                if model is not None:
                    print(f"Argument 'model' ({model}) ignored. Pass arguments directly!")

                model = OneClassSVM(**package_kwargs)
                ts_changepoints = [index for index, prediction in enumerate(model.fit_predict(df_[[column]])) if prediction == -1]

            elif package.lower() == "cpt":
                if not CPT_AVAILABLE:
                    results['error'] = 'changepoint package not available'
                    return results
                
                n = len(df_[column])

                if model is None:
                    model = "cpt.Bocpd()"
                elif model[-1] != ")":
                    model = model + "()"

                if model[:len("cpt.Bocpd")] == "cpt.Bocpd":  # Bocpd or BocpdTruncated
                    change_point_history = np.zeros((n, n))

                    if "prior" not in package_kwargs.keys():
                        package_kwargs.update({"prior": "cpt.NormalGamma()"})

                    if package_kwargs["prior"][-1] != ")":
                        package_kwargs["prior"] = package_kwargs["prior"] + "()"

                    if "lam" not in package_kwargs.keys():
                        package_kwargs.update({"lam": df_.shape[0]*0.1})
                else:  # ArgpCpd
                    change_point_history = np.zeros((n+1, n+1))

                model_str = model[::-1].replace(")", (", ".join([k + " = " + str(v) for k, v in package_kwargs.items()]) + ")")[::-1], 1)[::-1]
                model = eval(model_str)
                
                for i, x in enumerate(df_[column]):
                    if verbose:
                        print(f"i {i} / data.shape {df_.shape[0]}")

                    if model_str[:len("cpt.Bocpd")] == "cpt.Bocpd":
                        change_point_history[i, :i+1] = model.step(x)
                    else:
                        cps = model.step(x)
                        change_point_history[i, :len(cps)] = cps

                if verbose:
                    print("starting map")

                ts_changepoints = cpt.map_changepoints(change_point_history)

            else:
                results['error'] = f'Unknown package: {package}'
                return results

            # Convert indices to dates
            if hasattr(df, 'index') and len(ts_changepoints) > 0:
                shift_dates = [df.index[idx] for idx in ts_changepoints if idx < len(df)]
            else:
                shift_dates = ts_changepoints

            results['shifts'] = shift_dates
            results['details'] = {
                'package': package,
                'model': str(model) if model else None,
                'n_changepoints': len(ts_changepoints),
                'raw_indices': ts_changepoints
            }

        except Exception as e:
            results['error'] = str(e)

        return results
    def detect_all_shifts(self, products=None, methods=['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian'], 
                         cusum_params={'threshold': 3, 'drift': 0.5}, ts_assign_params=None):
        """Apply all shift detection methods including ts_assign_changepoints"""
        if products is None:
            # Select top products by data volume
            product_sizes = {}
            for product in self.df[self.product_col].unique():
                prod_data = self.df[self.df[self.product_col] == product]
                product_sizes[product] = len(prod_data)
            
            sorted_products = sorted(product_sizes.items(), key=lambda x: x[1], reverse=True)
            products = [prod for prod, _ in sorted_products[:10]]  # Top 10 products
            
        for product in products:
            self.shift_results[product] = {}
            product_data = self.df[self.df[self.product_col] == product].copy()
            
            if len(product_data) < 20:  # Skip products with insufficient data
                continue
                
            for target in self.target_vars:
                if target not in product_data.columns:
                    continue
                    
                series = product_data.set_index(self.date_col)[target].dropna()
                if len(series) < 20:
                    continue
                
                self.shift_results[product][target] = {}
                
                # Apply each method
                if 'cusum' in methods:
                    self.shift_results[product][target]['cusum'] = self._cusum_detection(
                        series, cusum_params['threshold'], cusum_params['drift'])
                
                if 'changepoint' in methods:
                    self.shift_results[product][target]['changepoint'] = self._changepoint_detection(series)
                
                if 'statistical' in methods:
                    self.shift_results[product][target]['statistical'] = self._statistical_tests(series)
                
                if 'ewma' in methods:
                    self.shift_results[product][target]['ewma'] = self._ewma_control(series)
                    
                if 'clustering' in methods:
                    self.shift_results[product][target]['clustering'] = self._time_series_clustering(series)
                
                if 'bayesian' in methods and BAYESIAN_AVAILABLE:
                    self.shift_results[product][target]['bayesian'] = self._bayesian_changepoint(series)
                
                # Add ts_assign methods if requested
                if ts_assign_params:
                    for ts_method in ts_assign_params:
                        method_name = f"ts_assign_{ts_method['package']}"
                        if method_name in methods:
                            # Create dataframe for ts_assign function
                            ts_df = product_data[[self.date_col, target]].copy()
                            ts_df = ts_df.set_index(self.date_col)
                            
                            self.shift_results[product][target][method_name] = self.ts_assign_changepoints(
                                ts_df, target, **ts_method)
    
    
    def _cusum_detection(self, series, threshold=3, drift=0.5):
        """
        CUSUM detection method - integrated from original code
        """
        results = {'shifts': {'positive_shifts': [], 'negative_shifts': []}, 
                  'method': 'cusum', 'cusum_stats': {}}
        
        try:
            # Standardize
            mean_val = series.mean()
            std_val = series.std()
            if std_val == 0:
                return results
            
            standardized = (series - mean_val) / std_val
            
            # Calculate CUSUM
            cusum_pos = np.zeros(len(standardized))
            cusum_neg = np.zeros(len(standardized))
            
            positive_shifts = []
            negative_shifts = []
            
            for j in range(1, len(standardized)):
                cusum_pos[j] = max(0, cusum_pos[j-1] + standardized.iloc[j] - drift)
                cusum_neg[j] = min(0, cusum_neg[j-1] + standardized.iloc[j] + drift)
                
                # Detect shifts
                if cusum_pos[j] > threshold and cusum_pos[j-1] <= threshold:
                    positive_shifts.append(j)
                if cusum_neg[j] < -threshold and cusum_neg[j-1] >= -threshold:
                    negative_shifts.append(j)
            
            # Convert indices to dates
            positive_shift_dates = [series.index[idx] for idx in positive_shifts if idx < len(series)]
            negative_shift_dates = [series.index[idx] for idx in negative_shifts if idx < len(series)]
            
            results['shifts'] = {
                'positive_shifts': positive_shift_dates,
                'negative_shifts': negative_shift_dates,
                'all_shifts': positive_shift_dates + negative_shift_dates
            }
            results['cusum_stats'] = {
                'cusum_pos': cusum_pos,
                'cusum_neg': cusum_neg,
                'threshold': threshold,
                'drift': drift,
                'mean': mean_val,
                'std': std_val
            }
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _changepoint_detection(self, series, methods=['pelt', 'window']):
        """
        Ruptures library changepoint detection - excellent for manufacturing data
        """
        results = {'shifts': [], 'method': 'changepoint', 'scores': []}
        
        try:
            signal = series.values
            
            # PELT algorithm for multiple changepoints
            algo_pelt = rpt.Pelt(model="rbf", min_size=5, jump=1).fit(signal)
            changepoints_pelt = algo_pelt.predict(pen=10)
            
            if changepoints_pelt and changepoints_pelt[-1] == len(signal):
                changepoints_pelt = changepoints_pelt[:-1]
            
            pelt_dates = [series.index[cp] for cp in changepoints_pelt if cp < len(series)]
            
            # Window-based detection
            algo_window = rpt.Window(width=10, model="l2").fit(signal)
            changepoints_window = algo_window.predict(n_bkps=3)
            if changepoints_window and changepoints_window[-1] == len(signal):
                changepoints_window = changepoints_window[:-1]
            
            window_dates = [series.index[cp] for cp in changepoints_window if cp < len(series)]
            
            results['shifts'] = {
                'pelt': pelt_dates,
                'window': window_dates,
                'combined': list(set(pelt_dates + window_dates))
            }
            results['scores'] = {'pelt_penalty': 10, 'n_changepoints': len(pelt_dates)}
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _statistical_tests(self, series, window_size=15):
        """
        Statistical shift detection using t-tests and variance tests
        """
        results = {'shifts': [], 'method': 'statistical', 'test_stats': []}
        
        try:
            values = series.values
            dates = series.index
            shifts = []
            test_stats = []
            
            for i in range(window_size, len(values) - window_size):
                before = values[i-window_size:i]
                after = values[i:i+window_size]
                
                # T-test for mean shift
                t_stat, t_p = stats.ttest_ind(before, after, equal_var=False)
                
                # F-test for variance shift
                f_stat = np.var(after, ddof=1) / np.var(before, ddof=1)
                f_p = 1 - stats.f.cdf(f_stat, len(after)-1, len(before)-1)
                
                if t_p < 0.01 or f_p < 0.01:
                    shifts.append(dates[i])
                    test_stats.append({
                        'date': dates[i],
                        't_stat': t_stat,
                        't_p': t_p,
                        'f_stat': f_stat,
                        'f_p': f_p,
                        'shift_magnitude': np.mean(after) - np.mean(before)
                    })
            
            results['shifts'] = shifts
            results['test_stats'] = test_stats
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _ewma_control(self, series, lambda_ewma=0.2, threshold_sigma=3):
        """
        Exponentially Weighted Moving Average control chart
        """
        results = {'shifts': [], 'method': 'ewma', 'control_stats': {}}
        
        try:
            values = series.values
            dates = series.index
            
            # Calculate EWMA
            ewma = np.zeros(len(values))
            ewma[0] = values[0]
            
            for i in range(1, len(values)):
                ewma[i] = lambda_ewma * values[i] + (1 - lambda_ewma) * ewma[i-1]
            
            # Control limits
            overall_mean = np.mean(values)
            overall_std = np.std(values)
            
            ewma_var = []
            for i in range(len(values)):
                var_i = (overall_std**2) * (lambda_ewma / (2 - lambda_ewma)) * (1 - (1 - lambda_ewma)**(2*(i+1)))
                ewma_var.append(var_i)
            
            ewma_std = np.sqrt(ewma_var)
            ucl = overall_mean + threshold_sigma * ewma_std
            lcl = overall_mean - threshold_sigma * ewma_std
            
            # Detect shifts
            shifts = []
            for i in range(1, len(ewma)):
                if ewma[i] > ucl[i] or ewma[i] < lcl[i]:
                    shifts.append(dates[i])
            
            results['shifts'] = shifts
            results['control_stats'] = {
                'ewma': ewma,
                'ucl': ucl,
                'lcl': lcl,
                'center_line': overall_mean,
                'lambda': lambda_ewma
            }
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _time_series_clustering(self, series, n_clusters=3, window_size=10):
        """
        Time series clustering approach to identify regime changes
        """
        results = {'shifts': [], 'method': 'clustering', 'cluster_info': {}}
        
        try:
            values = series.values
            dates = series.index
            
            windows = []
            window_dates = []
            
            for i in range(window_size, len(values)):
                window = values[i-window_size:i]
                windows.append([
                    np.mean(window),
                    np.std(window),
                    np.max(window) - np.min(window),
                    np.percentile(window, 75) - np.percentile(window, 25)
                ])
                window_dates.append(dates[i])
            
            if len(windows) < n_clusters:
                results['error'] = 'Insufficient data for clustering'
                return results
            
            scaler = StandardScaler()
            windows_scaled = scaler.fit_transform(windows)
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(windows_scaled)
            
            shifts = []
            for i in range(1, len(clusters)):
                if clusters[i] != clusters[i-1]:
                    shifts.append(window_dates[i])
            
            results['shifts'] = shifts
            results['cluster_info'] = {
                'clusters': clusters,
                'cluster_centers': kmeans.cluster_centers_,
                'silhouette_score': silhouette_score(windows_scaled, clusters),
                'n_clusters': n_clusters
            }
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _bayesian_changepoint(self, series, max_changepoints=5):
        """
        Bayesian changepoint detection using Student's t-test approach
        Lightweight implementation without requiring PyMC3
        """
        results = {'shifts': [], 'method': 'bayesian', 'probabilities': []}
        
        try:
            if not BAYESIAN_AVAILABLE:
                results['error'] = 'Bayesian libraries not available'
                return results
            
            values = series.values
            dates = series.index
            n = len(values)
            
            if n < 20:
                results['error'] = 'Insufficient data for Bayesian analysis'
                return results
            
            # Simple Bayesian changepoint detection using likelihood ratios
            changepoint_probs = []
            
            for t in range(10, n-10):  # Avoid edges
                # Calculate likelihood of no changepoint
                full_mean = np.mean(values)
                full_var = np.var(values, ddof=1)
                
                if full_var == 0:
                    continue
                
                # Log-likelihood for full series (no changepoint)
                ll_no_change = -0.5 * n * np.log(2 * np.pi * full_var) - 0.5 * np.sum((values - full_mean)**2) / full_var
                
                # Calculate likelihood with changepoint at t
                mean_1 = np.mean(values[:t])
                var_1 = np.var(values[:t], ddof=1) if t > 1 else full_var
                mean_2 = np.mean(values[t:])
                var_2 = np.var(values[t:], ddof=1) if n-t > 1 else full_var
                
                if var_1 == 0:
                    var_1 = full_var
                if var_2 == 0:
                    var_2 = full_var
                
                # Log-likelihood for two segments
                ll_1 = -0.5 * t * np.log(2 * np.pi * var_1) - 0.5 * np.sum((values[:t] - mean_1)**2) / var_1
                ll_2 = -0.5 * (n-t) * np.log(2 * np.pi * var_2) - 0.5 * np.sum((values[t:] - mean_2)**2) / var_2
                ll_change = ll_1 + ll_2
                
                # Bayes factor (simplified, without proper priors)
                bayes_factor = np.exp(ll_change - ll_no_change)
                
                # Convert to probability (rough approximation)
                prob_changepoint = bayes_factor / (1 + bayes_factor)
                changepoint_probs.append((t, prob_changepoint))
            
            # Select changepoints with high probability
            changepoint_probs.sort(key=lambda x: x[1], reverse=True)
            
            # Threshold for considering a changepoint significant
            threshold_prob = 0.7
            significant_changepoints = []
            
            for t, prob in changepoint_probs[:max_changepoints]:
                if prob > threshold_prob:
                    # Check that it's not too close to already selected changepoints
                    too_close = False
                    for existing_t, _ in significant_changepoints:
                        if abs(t - existing_t) < 10:  # Minimum 10 points apart
                            too_close = True
                            break
                    
                    if not too_close:
                        significant_changepoints.append((t, prob))
            
            # Convert to dates
            shifts = [dates[t] for t, prob in significant_changepoints]
            
            results['shifts'] = shifts
            results['probabilities'] = significant_changepoints
            results['all_probs'] = changepoint_probs
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def consensus_shifts(self, product, target, min_methods=2, tolerance_days=7):
        """
        Find shifts detected by multiple methods (consensus approach)
        Updated to handle ts_assign methods
        """
        if product not in self.shift_results or target not in self.shift_results[product]:
            return []
        
        all_shifts = []
        method_data = self.shift_results[product][target]
        
        # Collect all shift dates from different methods
        for method_name, method_result in method_data.items():
            if 'error' in method_result:
                continue
                
            if 'shifts' in method_result:
                if method_name == 'changepoint':
                    shifts = method_result['shifts'].get('combined', [])
                elif method_name == 'cusum':
                    shifts = method_result['shifts'].get('all_shifts', [])
                elif method_name.startswith('ts_assign_'):
                    shifts = method_result['shifts']
                elif isinstance(method_result['shifts'], list):
                    shifts = method_result['shifts']
                else:
                    continue
                all_shifts.extend(shifts)
        
        if not all_shifts:
            return []
        
        # Convert to pandas datetime
        shift_series = pd.to_datetime(all_shifts)
        
        # Group shifts within tolerance days of each other
        consensus_shifts = []
        tolerance = pd.Timedelta(days=tolerance_days)
        
        used_shifts = set()
        
        for shift in shift_series:
            if shift in used_shifts:
                continue
            
            # Find all shifts within tolerance
            nearby_shifts = shift_series[abs(shift_series - shift) <= tolerance]
            
            if len(nearby_shifts) >= min_methods:
                consensus_shifts.append(shift)
                used_shifts.update(nearby_shifts)
        
        return sorted(consensus_shifts)
    
    def plot_cusum_charts(self, product=None, target_var=None, threshold=3, drift=0.5, save_plots=True, max_products=5):
        """
        Create comprehensive CUSUM charts - integrated from original code
        """
        available_products = list(self.shift_results.keys())
        if product is None:
            product_sizes = {}
            for prod in available_products:
                prod_data = self.df[self.df[self.product_col] == prod]
                product_sizes[prod] = len(prod_data)
            
            sorted_products = sorted(product_sizes.items(), key=lambda x: x[1], reverse=True)
            products = [prod for prod, _ in sorted_products[:max_products]]
        else:
            products = [product] if product in available_products else available_products[:1]
        
        if target_var is None:
            targets = self.target_vars
        else:
            targets = [target_var] if target_var in self.target_vars else self.target_vars
        
        for target in targets:
            if target not in self.df.columns:
                continue
                
            n_products = len(products)
            if n_products == 0:
                continue
                
            fig, axes = plt.subplots(n_products, 3, figsize=(20, 5*n_products))
            if n_products == 1:
                axes = axes.reshape(1, -1)
            
            fig.suptitle(f'CUSUM Analysis for {target} (Top {n_products} Products by Data Volume)', 
                        fontsize=16, fontweight='bold')
            
            for i, product in enumerate(products):
                product_data = self.df[self.df[self.product_col] == product].copy()
                
                if target not in product_data.columns or product_data[target].isna().all():
                    continue
                
                series = product_data.set_index(self.date_col)[target].dropna()
                if len(series) < 10:
                    continue
                
                # Get CUSUM results
                if product in self.shift_results and target in self.shift_results[product] and 'cusum' in self.shift_results[product][target]:
                    cusum_result = self.shift_results[product][target]['cusum']
                    cusum_stats = cusum_result.get('cusum_stats', {})
                    cusum_pos = cusum_stats.get('cusum_pos', np.zeros(len(series)))
                    cusum_neg = cusum_stats.get('cusum_neg', np.zeros(len(series)))
                    mean_val = cusum_stats.get('mean', series.mean())
                    std_val = cusum_stats.get('std', series.std())
                else:
                    # Calculate CUSUM if not available
                    mean_val = series.mean()
                    std_val = series.std()
                    standardized = (series - mean_val) / std_val
                    cusum_pos = np.zeros(len(standardized))
                    cusum_neg = np.zeros(len(standardized))
                    
                    for j in range(1, len(standardized)):
                        cusum_pos[j] = max(0, cusum_pos[j-1] + standardized.iloc[j] - drift)
                        cusum_neg[j] = min(0, cusum_neg[j-1] + standardized.iloc[j] + drift)
                
                # Plot 1: Original Time Series
                ax1 = axes[i, 0]
                ax1.plot(series.index, series.values, 'b-', linewidth=1.5, label='Actual Data')
                ax1.axhline(y=mean_val, color='black', linestyle='-', alpha=0.8, label='Mean')
                ax1.axhline(y=mean_val + 3*std_val, color='red', linestyle='--', alpha=0.7, label='UCL (+3σ)')
                ax1.axhline(y=mean_val - 3*std_val, color='red', linestyle='--', alpha=0.7, label='LCL (-3σ)')
                
                # Mark detected shifts
                if (product in self.shift_results and target in self.shift_results[product] 
                    and 'cusum' in self.shift_results[product][target]):
                    shifts = self.shift_results[product][target]['cusum']['shifts']
                    for shift_date in shifts.get('positive_shifts', []):
                        ax1.axvline(shift_date, color='red', linestyle='--', alpha=0.8, linewidth=2)
                    for shift_date in shifts.get('negative_shifts', []):
                        ax1.axvline(shift_date, color='green', linestyle='--', alpha=0.8, linewidth=2)
                
                ax1.set_title(f'{product} - {target}\nOriginal Time Series (n={len(series)})')
                ax1.set_ylabel(target)
                ax1.legend(loc='upper right', fontsize=8)
                ax1.grid(True, alpha=0.3)
                plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
                
                # Plot 2: Positive CUSUM
                ax2 = axes[i, 1]
                ax2.plot(series.index, cusum_pos, 'r-', linewidth=2, label='Positive CUSUM')
                ax2.axhline(y=threshold, color='red', linestyle='--', alpha=0.8, label=f'Threshold ({threshold})')
                ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                
                above_threshold = cusum_pos > threshold
                if np.any(above_threshold):
                    ax2.fill_between(series.index, 0, cusum_pos, where=above_threshold, 
                                   color='red', alpha=0.3, label='Above Threshold')
                
                ax2.set_title(f'Positive CUSUM (Upward Shifts)\nDrift={drift}, Threshold={threshold}')
                ax2.set_ylabel('Positive CUSUM')
                ax2.legend(loc='upper right', fontsize=8)
                ax2.grid(True, alpha=0.3)
                plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
                
                # Plot 3: Negative CUSUM
                ax3 = axes[i, 2]
                ax3.plot(series.index, cusum_neg, 'g-', linewidth=2, label='Negative CUSUM')
                ax3.axhline(y=-threshold, color='green', linestyle='--', alpha=0.8, label=f'Threshold (-{threshold})')
                ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                
                below_threshold = cusum_neg < -threshold
                if np.any(below_threshold):
                    ax3.fill_between(series.index, 0, cusum_neg, where=below_threshold, 
                                   color='green', alpha=0.3, label='Below Threshold')
                
                ax3.set_title(f'Negative CUSUM (Downward Shifts)\nDrift={drift}, Threshold={threshold}')
                ax3.set_ylabel('Negative CUSUM')
                ax3.legend(loc='lower right', fontsize=8)
                ax3.grid(True, alpha=0.3)
                plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
            
            plt.tight_layout()
            if save_plots:
                plt.savefig(f'cusum_analysis_{target.replace(" ", "_").replace("%", "pct")}.png', 
                           dpi=300, bbox_inches='tight')
            plt.show()
    
    def plot_comprehensive_comparison(self, product, target, save_plots=True):
        """
        Plot comparison of ALL methods including CUSUM and Bayesian
        """
        if product not in self.shift_results or target not in self.shift_results[product]:
            print(f"No results found for {product} - {target}")
            return
        
        product_data = self.df[self.df[self.product_col] == product].copy()
        series = product_data.set_index(self.date_col)[target].dropna()
        
        if len(series) == 0:
            return
        
        # Create the plot with 6 subplots for all methods
        fig, axes = plt.subplots(6, 1, figsize=(16, 16))
        fig.suptitle(f'Comprehensive Shift Detection: {product} - {target}', fontsize=14, fontweight='bold')
        
        method_data = self.shift_results[product][target]
        
        # Plot 1: CUSUM
        ax1 = axes[0]
        ax1.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'cusum' in method_data and 'cusum_stats' in method_data['cusum']:
            cusum_stats = method_data['cusum']['cusum_stats']
            # Plot CUSUM on secondary axis
            ax1_twin = ax1.twinx()
            ax1_twin.plot(series.index, cusum_stats['cusum_pos'], 'r--', alpha=0.7, label='Pos CUSUM')
            ax1_twin.plot(series.index, cusum_stats['cusum_neg'], 'g--', alpha=0.7, label='Neg CUSUM')
            ax1_twin.axhline(y=cusum_stats['threshold'], color='red', linestyle=':', alpha=0.5)
            ax1_twin.axhline(y=-cusum_stats['threshold'], color='green', linestyle=':', alpha=0.5)
            ax1_twin.set_ylabel('CUSUM Values')
            
            for shift in method_data['cusum']['shifts']['all_shifts']:
                ax1.axvline(shift, color='red', linestyle='--', alpha=0.7, linewidth=2)
        
        ax1.set_title('CUSUM Detection')
        ax1.set_ylabel(target)
        ax1.legend(loc='upper left')
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Changepoint Detection
        ax2 = axes[1]
        ax2.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'changepoint' in method_data and 'shifts' in method_data['changepoint']:
            changepoints = method_data['changepoint']['shifts'].get('combined', [])
            for cp in changepoints:
                ax2.axvline(cp, color='red', linestyle='--', alpha=0.7, linewidth=2)
        
        ax2.set_title('Changepoint Detection (Ruptures)')
        ax2.set_ylabel(target)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Statistical Tests
        ax3 = axes[2]
        ax3.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'statistical' in method_data and 'shifts' in method_data['statistical']:
            for shift in method_data['statistical']['shifts']:
                ax3.axvline(shift, color='green', linestyle='--', alpha=0.7, linewidth=2)
        
        ax3.set_title('Statistical Tests (t-test & F-test)')
        ax3.set_ylabel(target)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: EWMA
        ax4 = axes[3]
        ax4.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'ewma' in method_data and 'control_stats' in method_data['ewma']:
            ewma_stats = method_data['ewma']['control_stats']
            ax4.plot(series.index, ewma_stats['ewma'], 'orange', linewidth=2, label='EWMA')
            ax4.plot(series.index, ewma_stats['ucl'], 'r--', alpha=0.7, label='UCL')
            ax4.plot(series.index, ewma_stats['lcl'], 'r--', alpha=0.7, label='LCL')
            
            for shift in method_data['ewma']['shifts']:
                ax4.axvline(shift, color='orange', linestyle='--', alpha=0.7, linewidth=2)
        
        ax4.set_title('EWMA Control Chart')
        ax4.set_ylabel(target)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # Plot 5: Clustering
        ax5 = axes[4]
        ax5.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'clustering' in method_data and 'shifts' in method_data['clustering']:
            for shift in method_data['clustering']['shifts']:
                ax5.axvline(shift, color='purple', linestyle='--', alpha=0.7, linewidth=2)
        
        ax5.set_title('Time Series Clustering')
        ax5.set_ylabel(target)
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # Plot 6: Bayesian + Consensus
        ax6 = axes[5]
        ax6.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'bayesian' in method_data and 'shifts' in method_data['bayesian']:
            for shift in method_data['bayesian']['shifts']:
                ax6.axvline(shift, color='brown', linestyle='--', alpha=0.7, linewidth=2, label='Bayesian')
        
        # Plot consensus shifts
        consensus = self.consensus_shifts(product, target)
        for shift in consensus:
            ax6.axvline(shift, color='black', linestyle='-', alpha=0.9, linewidth=3, label='Consensus')
        
        ax6.set_title('Bayesian Detection + Consensus Shifts')
        ax6.set_ylabel(target)
        ax6.set_xlabel('Date')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        if save_plots:
            filename = f'comprehensive_shift_analysis_{product}_{target}'.replace(' ', '_').replace('%', 'pct')
            plt.savefig(f'{filename}.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_method_performance_heatmap(self, save_plots=True):
        """
        Create heatmap showing number of shifts detected by each method for each product-target combination
        Updated to include ts_assign methods
        """
        # Collect data for heatmap
        products = list(self.shift_results.keys())
        
        # Get all available methods dynamically
        all_methods = set()
        for product in products:
            for target in self.target_vars:
                if target in self.shift_results[product]:
                    all_methods.update(self.shift_results[product][target].keys())
        
        # Sort methods for consistent display
        base_methods = ['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian']
        ts_methods = [m for m in all_methods if m.startswith('ts_assign_')]
        methods = base_methods + sorted(ts_methods)
        methods = [m for m in methods if m in all_methods]  # Only include available methods
        
        heatmap_data = []
        labels = []
        
        for product in products:
            for target in self.target_vars:
                if target in self.shift_results[product]:
                    row = []
                    for method in methods:
                        if method in self.shift_results[product][target]:
                            method_result = self.shift_results[product][target][method]
                            if 'error' in method_result:
                                count = 0
                            elif method == 'cusum':
                                count = len(method_result['shifts'].get('all_shifts', []))
                            elif method == 'changepoint':
                                count = len(method_result['shifts'].get('combined', []))
                            else:
                                count = len(method_result.get('shifts', []))
                        else:
                            count = 0
                        row.append(count)
                    
                    # Add consensus count
                    consensus_count = len(self.consensus_shifts(product, target))
                    row.append(consensus_count)
                    
                    heatmap_data.append(row)
                    labels.append(f'{product[:8]}_{target}')
        
        if not heatmap_data:
            print("No data available for heatmap")
            return
        
        # Create heatmap
        fig, ax = plt.subplots(figsize=(max(10, len(methods)*0.8), max(8, len(labels)*0.3)))
        
        heatmap_array = np.array(heatmap_data)
        im = ax.imshow(heatmap_array, cmap='YlOrRd', aspect='auto')
        
        # Set ticks and labels
        ax.set_xticks(range(len(methods) + 1))
        ax.set_xticklabels(methods + ['Consensus'], rotation=45, ha='right')
        ax.set_yticks(range(len(labels)))
        ax.set_yticklabels(labels)
        
        # Add text annotations
        for i in range(len(labels)):
            for j in range(len(methods) + 1):
                text = ax.text(j, i, int(heatmap_array[i, j]), 
                             ha="center", va="center", color="black", fontsize=8)
        
        ax.set_title('Number of Shifts Detected by Method\n(Product_Target combinations)', fontweight='bold')
        plt.colorbar(im)
        plt.tight_layout()
        
        if save_plots:
            plt.savefig('method_performance_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def summary_report(self):
        """
        Generate comprehensive summary report of all detected shifts
        """
        report = []
        
        for product in self.shift_results:
            for target in self.shift_results[product]:
                consensus_shifts = self.consensus_shifts(product, target)
                
                method_counts = {}
                method_errors = {}
                
                for method_name, method_result in self.shift_results[product][target].items():
                    if 'error' in method_result:
                        method_errors[method_name] = method_result['error']
                        method_counts[method_name] = 0
                    elif 'shifts' in method_result:
                        if method_name == 'cusum':
                            count = len(method_result['shifts'].get('all_shifts', []))
                        elif method_name == 'changepoint':
                            count = len(method_result['shifts'].get('combined', []))
                        else:
                            count = len(method_result['shifts'])
                        method_counts[method_name] = count
                
                # Calculate data quality metrics
                product_data = self.df[self.df[self.product_col] == product]
                series = product_data.set_index(self.date_col)[target].dropna()
                
                report.append({
                    'Product': product,
                    'Target': target,
                    'Data_Points': len(series),
                    'Date_Range': f"{series.index.min()} to {series.index.max()}",
                    'Consensus_Shifts': len(consensus_shifts),
                    'Consensus_Dates': [str(d.date()) for d in consensus_shifts],
                    **{f'{method}_count': method_counts.get(method, 0) 
                       for method in ['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian']},
                    'Errors': '; '.join([f'{k}: {v}' for k, v in method_errors.items()]) if method_errors else 'None'
                })
        
        return pd.DataFrame(report)
    
    def correlate_shifts_with_variables(self, window_days=7, consensus_only=True, min_methods=2):
        """
        Correlate detected shifts with process/material/prestep variable changes
        Updated to work with comprehensive shift detection results
        """
        print("Correlating shifts with variable changes...")
        
        correlation_results = {}
        
        for product, product_shifts in self.shift_results.items():
            product_data = self.df[self.df[self.product_col] == product].copy()
            product_correlations = {}
            
            for target, method_results in product_shifts.items():
                # Collect all shifts from different methods
                all_shifts = []
                
                if consensus_only:
                    # Use only consensus shifts for higher confidence
                    consensus_shifts = self.consensus_shifts(product, target, min_methods=min_methods)
                    all_shifts = consensus_shifts
                else:
                    # Use all detected shifts from all methods
                    for method_name, method_result in method_results.items():
                        if 'error' in method_result:
                            continue
                            
                        if 'shifts' in method_result:
                            if method_name == 'cusum':
                                shifts = method_result['shifts'].get('all_shifts', [])
                            elif method_name == 'changepoint':
                                shifts = method_result['shifts'].get('combined', [])
                            elif method_name.startswith('ts_assign_'):
                                shifts = method_result['shifts']
                            elif isinstance(method_result['shifts'], list):
                                shifts = method_result['shifts']
                            else:
                                continue
                            all_shifts.extend(shifts)
                
                if not all_shifts:
                    continue
                
                # Convert dates to indices for analysis
                product_data_indexed = product_data.set_index(self.date_col)
                shift_indices = []
                
                for shift_date in all_shifts:
                    try:
                        # Find the closest index to the shift date
                        shift_date = pd.to_datetime(shift_date)
                        if shift_date in product_data_indexed.index:
                            shift_idx = product_data_indexed.index.get_loc(shift_date)
                            shift_indices.append(shift_idx)
                    except (KeyError, ValueError):
                        continue
                
                if shift_indices:
                    # Analyze variable changes around shift points
                    variable_correlations = self.analyze_variable_changes_around_shifts(
                        product_data_indexed, shift_indices, window_days)
                    
                    product_correlations[target] = {
                        'variable_correlations': variable_correlations,
                        'shift_count': len(shift_indices),
                        'shift_dates': all_shifts,
                        'analysis_method': 'consensus' if consensus_only else 'all_methods'
                    }
            
            correlation_results[product] = product_correlations
        
        self.correlation_results = correlation_results
        return correlation_results
    
    def analyze_variable_changes_around_shifts(self, data, shift_indices, window_days=7):
        """
        Analyze how variables change around shift points
        Updated to work with datetime-indexed data
        """
        variable_importance = {}
        
        # Define variable categories based on your description
        material_vars = ['X11', 'X12', 'X13', 'X14', 'X15', 'X17', 'X24'] + [f'X{i}' for i in range(38, 45)]
        prestep_vars = ['X5', 'X6', 'X7', 'X8', 'X18', 'X19', 'X25', 'X26', 'X28', 'X29', 'X34', 'X35', 'X36']
        process_vars = [f'X{i}' for i in range(1, 34) if f'X{i}' not in material_vars + prestep_vars]
        
        # Get all potential predictor variables including encoded categorical and lagged variables
        all_vars = material_vars + prestep_vars + process_vars
        all_vars += [col for col in data.columns if '_lag' in col or 'X14_' in col or 'X17_' in col]
        all_vars = [var for var in all_vars if var in data.columns]
        
        for var in all_vars:
            # Skip non-numeric variables
            if data[var].dtype in ['object', 'string'] or var in ['X14', 'X17']:
                continue
                
            changes = []
            valid_shifts = 0
            
            for shift_idx in shift_indices:
                # Ensure we have enough data before and after the shift
                if shift_idx < window_days or shift_idx >= len(data) - window_days:
                    continue
                
                try:
                    before_data = data.iloc[shift_idx-window_days:shift_idx][var]
                    after_data = data.iloc[shift_idx:shift_idx+window_days][var]
                    
                    # Calculate means, handling missing values
                    before = before_data.dropna().mean()
                    after = after_data.dropna().mean()
                    
                    if pd.notna(before) and pd.notna(after) and before != 0:
                        # Calculate percentage change
                        pct_change = abs((after - before) / before) * 100
                        changes.append(pct_change)
                        valid_shifts += 1
                        
                        # Also calculate absolute change for context
                        abs_change = abs(after - before)
                        
                except (IndexError, ValueError):
                    continue
            
            if changes and valid_shifts > 0:
                # Calculate statistics for the variable
                mean_change = np.mean(changes)
                max_change = np.max(changes)
                std_change = np.std(changes) if len(changes) > 1 else 0
                frequency = valid_shifts / len(shift_indices) if shift_indices else 0
                
                # Determine variable category for context
                var_category = 'process'
                if var in material_vars or '_lag' in var:
                    var_category = 'material'
                elif var in prestep_vars:
                    var_category = 'prestep'
                
                variable_importance[var] = {
                    'mean_change_pct': mean_change,
                    'max_change_pct': max_change,
                    'std_change_pct': std_change,
                    'frequency': frequency,
                    'valid_shifts': valid_shifts,
                    'total_shifts': len(shift_indices),
                    'category': var_category,
                    'importance_score': mean_change * frequency,  # Combined score
                    'reliability_score': frequency * (1 - std_change / (mean_change + 1e-6))  # Accounts for consistency
                }
        
        # Sort by importance score (combination of magnitude and frequency)
        sorted_vars = sorted(variable_importance.items(), 
                           key=lambda x: x[1]['importance_score'], reverse=True)
        
        return dict(sorted_vars[:25])  # Top 25 most important variables
    
    def plot_shift_correlation_analysis(self, product, target, save_plots=True, top_n_vars=10):
        """
        Visualize correlation between shifts and variable changes
        """
        if (product not in self.correlation_results or 
            target not in self.correlation_results[product]):
            print(f"No correlation results found for {product} - {target}")
            return
        
        corr_data = self.correlation_results[product][target]
        var_correlations = corr_data['variable_correlations']
        shift_dates = corr_data['shift_dates']
        
        if not var_correlations:
            print(f"No variable correlations found for {product} - {target}")
            return
        
        # Get top variables
        top_vars = list(var_correlations.keys())[:top_n_vars]
        
        # Create visualization
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'Shift-Variable Correlation Analysis: {product} - {target}', 
                    fontsize=14, fontweight='bold')
        
        # Plot 1: Variable Importance Scores
        ax1 = axes[0, 0]
        vars_names = [var[:15] + '...' if len(var) > 15 else var for var in top_vars]
        importance_scores = [var_correlations[var]['importance_score'] for var in top_vars]
        colors = ['red' if var_correlations[var]['category'] == 'material' 
                 else 'blue' if var_correlations[var]['category'] == 'prestep' 
                 else 'green' for var in top_vars]
        
        bars = ax1.barh(vars_names, importance_scores, color=colors, alpha=0.7)
        ax1.set_xlabel('Importance Score (Mean Change % × Frequency)')
        ax1.set_title('Top Variables by Importance Score')
        ax1.grid(True, alpha=0.3, axis='x')
        
        # Add legend for categories
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='red', alpha=0.7, label='Material'),
                          Patch(facecolor='blue', alpha=0.7, label='Pre-step'),
                          Patch(facecolor='green', alpha=0.7, label='Process')]
        ax1.legend(handles=legend_elements, loc='lower right')
        
        # Plot 2: Frequency vs Mean Change scatter
        ax2 = axes[0, 1]
        for var in top_vars:
            stats = var_correlations[var]
            color = 'red' if stats['category'] == 'material' else 'blue' if stats['category'] == 'prestep' else 'green'
            ax2.scatter(stats['frequency'], stats['mean_change_pct'], 
                       color=color, alpha=0.7, s=100)
            ax2.annotate(var[:8], (stats['frequency'], stats['mean_change_pct']), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        ax2.set_xlabel('Frequency (shifts with this variable change)')
        ax2.set_ylabel('Mean Change (%)')
        ax2.set_title('Variable Change Frequency vs Magnitude')
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Timeline with shifts and key variable
        ax3 = axes[1, 0]
        product_data = self.df[self.df[self.product_col] == product].copy()
        series = product_data.set_index(self.date_col)[target].dropna()
        
        ax3.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label=target)
        
        # Mark shift points
        for shift_date in shift_dates:
            ax3.axvline(pd.to_datetime(shift_date), color='red', linestyle='--', alpha=0.7, linewidth=2)
        
        ax3.set_title(f'{target} Time Series with Detected Shifts')
        ax3.set_ylabel(target)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
        
        # Plot 4: Variable category breakdown
        ax4 = axes[1, 1]
        category_counts = {}
        category_avg_importance = {}
        
        for var, stats in var_correlations.items():
            cat = stats['category']
            if cat not in category_counts:
                category_counts[cat] = 0
                category_avg_importance[cat] = []
            category_counts[cat] += 1
            category_avg_importance[cat].append(stats['importance_score'])
        
        categories = list(category_counts.keys())
        counts = list(category_counts.values())
        avg_importance = [np.mean(category_avg_importance[cat]) for cat in categories]
        
        # Create dual bar chart
        x_pos = np.arange(len(categories))
        ax4_twin = ax4.twinx()
        
        bars1 = ax4.bar(x_pos - 0.2, counts, 0.4, label='Count', alpha=0.7, color='lightblue')
        bars2 = ax4_twin.bar(x_pos + 0.2, avg_importance, 0.4, label='Avg Importance', alpha=0.7, color='orange')
        
        ax4.set_xlabel('Variable Categories')
        ax4.set_ylabel('Number of Important Variables', color='blue')
        ax4_twin.set_ylabel('Average Importance Score', color='orange')
        ax4.set_title('Variable Importance by Category')
        ax4.set_xticks(x_pos)
        ax4.set_xticklabels(categories)
        
        # Add value labels on bars
        for i, (count, avg_imp) in enumerate(zip(counts, avg_importance)):
            ax4.text(i - 0.2, count + 0.1, str(count), ha='center', va='bottom')
            ax4_twin.text(i + 0.2, avg_imp + 0.1, f'{avg_imp:.1f}', ha='center', va='bottom')
        
        plt.tight_layout()
        if save_plots:
            filename = f'shift_correlation_analysis_{product}_{target}'.replace(' ', '_').replace('%', 'pct')
            plt.savefig(f'{filename}.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_correlation_summary_report(self):
        """
        Generate summary report of shift-variable correlations
        """
        if not hasattr(self, 'correlation_results') or not self.correlation_results:
            print("No correlation results available. Run correlate_shifts_with_variables() first.")
            return pd.DataFrame()
        
        report_data = []
        
        for product, product_data in self.correlation_results.items():
            for target, target_data in product_data.items():
                var_correlations = target_data['variable_correlations']
                
                if not var_correlations:
                    continue
                
                # Get top 5 most important variables
                top_vars = list(var_correlations.keys())[:5]
                
                for rank, var in enumerate(top_vars, 1):
                    stats = var_correlations[var]
                    
                    report_data.append({
                        'Product': product,
                        'Target': target,
                        'Variable': var,
                        'Rank': rank,
                        'Category': stats['category'],
                        'Importance_Score': round(stats['importance_score'], 2),
                        'Mean_Change_Pct': round(stats['mean_change_pct'], 2),
                        'Frequency': round(stats['frequency'], 2),
                        'Valid_Shifts': stats['valid_shifts'],
                        'Total_Shifts': stats['total_shifts']
                    })
        
        return pd.DataFrame(report_data)
        """
        Export detailed results to Excel with multiple sheets
        """
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # Summary sheet
            summary = self.summary_report()
            summary.to_excel(writer, sheet_name='Summary', index=False)
            
            # Detailed results for each method
            for product in self.shift_results:
                for target in self.shift_results[product]:
                    sheet_name = f'{product[:10]}_{target[:10]}'.replace(' ', '_').replace('%', 'pct')
                    
                    detailed_data = []
                    
                    for method_name, method_result in self.shift_results[product][target].items():
                        if 'error' in method_result:
                            detailed_data.append({
                                'Method': method_name,
                                'Status': 'Error',
                                'Details': method_result['error'],
                                'Shift_Count': 0,
                                'Shift_Dates': ''
                            })
                        elif 'shifts' in method_result:
                            if method_name == 'cusum':
                                shifts = method_result['shifts'].get('all_shifts', [])
                            elif method_name == 'changepoint':
                                shifts = method_result['shifts'].get('combined', [])
                            else:
                                shifts = method_result.get('shifts', [])
                            
                            detailed_data.append({
                                'Method': method_name,
                                'Status': 'Success',
                                'Details': f"Detected {len(shifts)} shifts",
                                'Shift_Count': len(shifts),
                                'Shift_Dates': '; '.join([str(d.date()) if hasattr(d, 'date') else str(d) for d in shifts])
                            })
                    
                    # Add consensus row
                    consensus = self.consensus_shifts(product, target)
                    detailed_data.append({
                        'Method': 'Consensus',
                        'Status': 'Computed',
                        'Details': f"Consensus from {len(detailed_data)} methods",
                        'Shift_Count': len(consensus),
                        'Shift_Dates': '; '.join([str(d.date()) for d in consensus])
                    })
                    
                    if detailed_data:
                        detail_df = pd.DataFrame(detailed_data)
    def export_shift_details(self, filename='comprehensive_shift_analysis.xlsx'):
        """
        Export detailed results to Excel with multiple sheets including correlation analysis
        """
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # Summary sheet
            summary = self.summary_report()
            summary.to_excel(writer, sheet_name='Summary', index=False)
            
            # Correlation summary sheet
            if hasattr(self, 'correlation_results') and self.correlation_results:
                corr_summary = self.generate_correlation_summary_report()
                if not corr_summary.empty:
                    corr_summary.to_excel(writer, sheet_name='Correlation_Summary', index=False)
            
            # Detailed results for each method
            for product in self.shift_results:
                for target in self.shift_results[product]:
                    sheet_name = f'{product[:10]}_{target[:10]}'.replace(' ', '_').replace('%', 'pct')
                    
                    detailed_data = []
                    
                    for method_name, method_result in self.shift_results[product][target].items():
                        if 'error' in method_result:
                            detailed_data.append({
                                'Method': method_name,
                                'Status': 'Error',
                                'Details': method_result['error'],
                                'Shift_Count': 0,
                                'Shift_Dates': ''
                            })
                        elif 'shifts' in method_result:
                            if method_name == 'cusum':
                                shifts = method_result['shifts'].get('all_shifts', [])
                            elif method_name == 'changepoint':
                                shifts = method_result['shifts'].get('combined', [])
                            elif method_name.startswith('ts_assign_'):
                                shifts = method_result.get('shifts', [])
                            else:
                                shifts = method_result.get('shifts', [])
                            
                            detailed_data.append({
                                'Method': method_name,
                                'Status': 'Success',
                                'Details': f"Detected {len(shifts)} shifts",
                                'Shift_Count': len(shifts),
                                'Shift_Dates': '; '.join([str(d.date()) if hasattr(d, 'date') else str(d) for d in shifts])
                            })
                    
                    # Add consensus row
                    consensus = self.consensus_shifts(product, target)
                    detailed_data.append({
                        'Method': 'Consensus',
                        'Status': 'Computed',
                        'Details': f"Consensus from {len(detailed_data)} methods",
                        'Shift_Count': len(consensus),
                        'Shift_Dates': '; '.join([str(d.date()) for d in consensus])
                    })
                    
                    if detailed_data:
                        detail_df = pd.DataFrame(detailed_data)
                        detail_df.to_excel(writer, sheet_name=sheet_name, index=False)


# Enhanced example usage with correlation analysis
"""
# Initialize the comprehensive shift detection suite
detector = ComprehensiveShiftDetection(df, target_vars=['Y1', 'Y2', 'Y3', 'Y4', 'Y5'])

# Define ts_assign methods to include
ts_assign_methods = [
    {'package': 'rb', 'season': 'none', 'quiet': 1},
    {'package': 'rpt', 'model': 'rpt.Pelt(model="l2", min_size=5)', 'pen': 0.01},
    {'package': 'ocsvm', 'kernel': 'rbf', 'gamma': 'scale', 'nu': 0.05}
]

# Run comprehensive shift detection
methods_to_use = ['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian'] + \
                 ['ts_assign_rb', 'ts_assign_rpt', 'ts_assign_ocsvm']

detector.detect_all_shifts(
    methods=methods_to_use,
    ts_assign_params=ts_assign_methods
)

# STEP 1: Generate shift detection summary
summary = detector.summary_report()
print("SHIFT DETECTION SUMMARY:")
print(summary.to_string())

# STEP 2: Correlate shifts with variable changes
print("\n" + "="*50)
print("CORRELATING SHIFTS WITH VARIABLE CHANGES...")
print("="*50)

# Run correlation analysis using consensus shifts (higher confidence)
correlation_results = detector.correlate_shifts_with_variables(
    window_days=7, 
    consensus_only=True, 
    min_methods=3
)

# Generate correlation summary report
corr_summary = detector.generate_correlation_summary_report()
print("\nTOP VARIABLE CORRELATIONS WITH SHIFTS:")
print(corr_summary.head(20).to_string())

# STEP 3: Visualize correlations for top products
top_products = summary.nlargest(3, 'Data_Points')['Product'].unique()

for product in top_products[:2]:  # Focus on top 2 products
    for target in ['Y1', 'Y2']:  # Key targets
        print(f"\n=== ANALYZING {product} - {target} ===")
        
        # Plot correlation analysis
        detector.plot_shift_correlation_analysis(product, target)
        
        # Print detailed correlation info
        if (product in correlation_results and 
            target in correlation_results[product]):
            
            var_corr = correlation_results[product][target]['variable_correlations']
            print(f"\nTop 5 correlated variables for {product} - {target}:")
            
            for i, (var, stats) in enumerate(list(var_corr.items())[:5], 1):
                print(f"{i}. {var} ({stats['category']})")
                print(f"   - Importance Score: {stats['importance_score']:.2f}")
                print(f"   - Mean Change: {stats['mean_change_pct']:.1f}%")
                print(f"   - Frequency: {stats['frequency']:.2f}")

# STEP 4: Export comprehensive results
detector.export_shift_details('manufacturing_shift_analysis_with_correlations.xlsx')

# STEP 5: Create performance heatmap
detector.plot_method_performance_heatmap()

print("\n" + "="*50)
print("ANALYSIS COMPLETE!")
print("="*50)
print("Files generated:")
print("- manufacturing_shift_analysis_with_correlations.xlsx")
print("- Various PNG plots for shift detection and correlations")
print("\nKey insights:")
print("- Consensus shifts provide high-confidence change points")
print("- Variable correlations identify potential root causes")
print("- Material variables may show 2-7 day lag effects")
print("- Pre-step variables may show 1 day lag effects")
"""
"""
# Initialize the comprehensive shift detection suite
detector = ComprehensiveShiftDetection(df, target_vars=['Y1', 'Y2', 'Y3', 'Y4', 'Y5'])

# Define ts_assign methods to include
ts_assign_methods = [
    {'package': 'rb', 'season': 'none', 'quiet': 1},  # Rbeast method
    {'package': 'rpt', 'model': 'rpt.Pelt(model="l2", min_size=5)', 'pen': 0.01},  # Ruptures PELT
    {'package': 'ocsvm', 'kernel': 'rbf', 'gamma': 'scale', 'nu': 0.05},  # OneClassSVM
    {'package': 'cpt', 'model': 'cpt.Bocpd', 'prior': 'cpt.NormalGamma', 'lam': 10}  # Changepoint package
]

# Run comprehensive analysis with all methods including ts_assign
methods_to_use = ['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian'] + \
                 [f'ts_assign_{m["package"]}' for m in ts_assign_methods]

detector.detect_all_shifts(
    methods=methods_to_use,
    ts_assign_params=ts_assign_methods
)

# Generate enhanced summary report
summary = detector.summary_report()
print("COMPREHENSIVE SHIFT DETECTION SUMMARY:")
print(summary.to_string())

# Create performance heatmap with all methods
detector.plot_method_performance_heatmap()

# Example focused analysis on top products
top_products = summary.nlargest(3, 'Data_Points')['Product'].unique()

# Compare all methods for key targets
for product in top_products[:2]:
    for target in ['Y1', 'Y2']:
        print(f"\n=== {product} - {target} ===")
        
        # Get results from each method
        if product in detector.shift_results and target in detector.shift_results[product]:
            for method, result in detector.shift_results[product][target].items():
                if 'error' not in result:
                    n_shifts = len(result.get('shifts', []))
                    print(f"{method:15}: {n_shifts:2d} shifts")
        
        # Get consensus
        consensus = detector.consensus_shifts(product, target, min_methods=3)
        print(f"{'Consensus':15}: {len(consensus):2d} shifts")
        
        if consensus:
            print("Consensus dates:", [d.strftime('%Y-%m-%d') for d in consensus])

# Create comprehensive comparison plots
for product in top_products[:1]:  # Focus on top product
    detector.plot_comprehensive_comparison(product, 'Y1')

# Export all results including ts_assign methods
detector.export_shift_details('comprehensive_analysis_with_ts_assign.xlsx')

print("\nMETHOD CAPABILITIES:")
print("✓ CUSUM: Traditional cumulative sum control charts")
print("✓ Ruptures: Advanced changepoint detection (PELT, Window)")  
print("✓ Statistical: t-tests and F-tests for mean/variance shifts")
print("✓ EWMA: Exponentially weighted moving average control charts")
print("✓ Clustering: Time series regime change detection")
print("✓ Bayesian: Probabilistic changepoint detection")
if RBEAST_AVAILABLE:
    print("✓ Rbeast: Fast automatic trend/seasonal/changepoint decomposition")
else:
    print("✗ Rbeast: Not available (install Rbeast)")
print("✓ OneClassSVM: Anomaly-based shift detection")
if CPT_AVAILABLE:
    print("✓ Changepoint pkg: Bayesian online changepoint detection")
else:
    print("✗ Changepoint pkg: Not available (install changepoint)")
"""
