import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy import stats
from sklearn.model_selection import TimeSeriesSplit
from sklearn.impute import KNNImputer
import plotly.graph_objects as go
from plotly.subplots import make_subplots
warnings.filterwarnings('ignore')

class OEEAnalyzer:
    def __init__(self, data_path=None, df=None):
        """
        Initialize OEE Analyzer
        Args:
            data_path: Path to CSV file
            df: DataFrame if already loaded
        """
        if df is not None:
            self.df = df.copy()
        elif data_path:
            self.df = pd.read_csv(data_path)
        else:
            raise ValueError("Either data_path or df must be provided")
            
        # Define variable categories
        self.target_vars = ['Y1', 'Y2', 'Y3', 'Y4', 'Y5']
        self.material_vars = ['X11', 'X12', 'X13', 'X14', 'X15', 'X17', 'X24'] + [f'X{i}' for i in range(38, 45)]
        self.prestep_vars = ['X5', 'X6', 'X7', 'X8', 'X18', 'X19', 'X25', 'X26', 'X28', 'X29', 'X34', 'X35', 'X36']
        
        # Process variables (X1-X33 excluding material and prestep vars)
        all_x_vars = [f'X{i}' for i in range(1, 34)]
        self.process_vars = [var for var in all_x_vars if var not in self.material_vars + self.prestep_vars]
        
        self.string_vars = ['X14', 'X17']
        self.date_col = 'GMT Prod Yield Date'
        self.product_col = 'Merge'
        
        self.results = {}
        
    def preprocess_data(self):
        """Comprehensive data preprocessing"""
        print("Starting data preprocessing...")
        
        # Convert date column
        self.df[self.date_col] = pd.to_datetime(self.df[self.date_col])
        self.df = self.df.sort_values(self.date_col).reset_index(drop=True)
        
        # Handle string variables
        le_dict = {}
        for var in self.string_vars:
            if var in self.df.columns:
                le = LabelEncoder()
                # Handle missing values in string columns
                self.df[var] = self.df[var].fillna('Unknown')
                self.df[var] = le.fit_transform(self.df[var].astype(str))
                le_dict[var] = le
        
        # Create lagged features for material variables (2-7 days lag)
        print("Creating lagged features for material variables...")
        for var in self.material_vars:
            if var in self.df.columns:
                for lag in range(2, 8):  # 2-7 days lag
                    self.df[f'{var}_lag{lag}'] = self.df[var].shift(lag)
        
        # Create lagged features for prestep variables (0-1 day lag)
        print("Creating lagged features for prestep variables...")
        for var in self.prestep_vars:
            if var in self.df.columns:
                self.df[f'{var}_lag1'] = self.df[var].shift(1)
        
        # Handle missing values with KNN imputation for numeric columns
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col not in [self.date_col]]
        
        print("Imputing missing values...")
        imputer = KNNImputer(n_neighbors=5)
        self.df[numeric_cols] = imputer.fit_transform(self.df[numeric_cols])
        
        print("Preprocessing completed.")
        return self
    
    def detect_shifts(self, window_size=30, significance_level=0.05):
        """
        Detect shifts in target variables by product type
        Focus on downward trends as specified
        """
        print("Detecting shifts in target variables...")
        shifts_detected = {}
        
        for product in self.df[self.product_col].unique():
            product_data = self.df[self.df[self.product_col] == product].copy()
            
            if len(product_data) < window_size * 2:
                continue
                
            shifts_detected[product] = {}
            
            for target in self.target_vars:
                if target in product_data.columns:
                    # Calculate rolling statistics
                    product_data[f'{target}_rolling_mean'] = product_data[target].rolling(window=window_size).mean()
                    product_data[f'{target}_rolling_std'] = product_data[target].rolling(window=window_size).std()
                    
                    # Detect significant downward shifts using t-test
                    shifts = []
                    for i in range(window_size, len(product_data) - window_size):
                        before = product_data[target].iloc[i-window_size:i]
                        after = product_data[target].iloc[i:i+window_size]
                        
                        # Focus on downward shifts (before > after)
                        if before.mean() > after.mean():
                            t_stat, p_value = stats.ttest_ind(before.dropna(), after.dropna())
                            if p_value < significance_level and t_stat > 0:
                                shifts.append({
                                    'date': product_data.iloc[i][self.date_col],
                                    'before_mean': before.mean(),
                                    'after_mean': after.mean(),
                                    'magnitude': before.mean() - after.mean(),
                                    'p_value': p_value
                                })
                    
                    shifts_detected[product][target] = shifts
        
        self.shifts = shifts_detected
        return shifts_detected
    
    def prepare_features(self, product_type):
        """Prepare feature matrix for specific product type"""
        product_data = self.df[self.df[self.product_col] == product_type].copy()
        
        # Get all potential features
        feature_cols = []
        
        # Original variables
        for var_list in [self.process_vars, self.material_vars, self.prestep_vars]:
            feature_cols.extend([col for col in var_list if col in product_data.columns])
        
        # Lagged variables
        lagged_cols = [col for col in product_data.columns if '_lag' in col]
        feature_cols.extend(lagged_cols)
        
        # Remove any non-numeric columns and date columns
        feature_cols = [col for col in feature_cols if col in product_data.select_dtypes(include=[np.number]).columns]
        
        return product_data, feature_cols
    
    def time_series_split(self, X, y, n_splits=5):
        """Create time series splits for validation"""
        tscv = TimeSeriesSplit(n_splits=n_splits)
        return tscv.split(X)
    
    def train_models(self, X_train, X_test, y_train, y_test):
        """Train multiple models and return results"""
        models = {}
        predictions = {}
        scores = {}
        
        # Random Forest
        rf = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=3,
            random_state=42,
            n_jobs=-1
        )
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)
        
        models['Random Forest'] = rf
        predictions['Random Forest'] = rf_pred
        scores['Random Forest'] = {
            'MAE': mean_absolute_error(y_test, rf_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),
            'R2': r2_score(y_test, rf_pred)
        }
        
        # XGBoost
        xgb_model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        xgb_model.fit(X_train, y_train)
        xgb_pred = xgb_model.predict(X_test)
        
        models['XGBoost'] = xgb_model
        predictions['XGBoost'] = xgb_pred
        scores['XGBoost'] = {
            'MAE': mean_absolute_error(y_test, xgb_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),
            'R2': r2_score(y_test, xgb_pred)
        }
        
        return models, predictions, scores
    
    def calculate_feature_importance(self, models, X_train, X_test, y_test, feature_names):
        """Calculate various feature importance metrics"""
        importance_results = {}
        
        for model_name, model in models.items():
            importance_results[model_name] = {}
            
            # Built-in feature importance
            if hasattr(model, 'feature_importances_'):
                importance_results[model_name]['built_in'] = dict(zip(feature_names, model.feature_importances_))
            
            # Permutation importance
            perm_importance = permutation_importance(
                model, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1
            )
            importance_results[model_name]['permutation'] = dict(zip(feature_names, perm_importance.importances_mean))
            importance_results[model_name]['permutation_std'] = dict(zip(feature_names, perm_importance.importances_std))
        
        return importance_results
    
    def analyze_product(self, product_type, target_var):
        """Comprehensive analysis for specific product and target"""
        print(f"\nAnalyzing {product_type} - {target_var}")
        
        # Prepare data
        product_data, feature_cols = self.prepare_features(product_type)
        
        if len(product_data) < 50:  # Minimum data requirement
            print(f"Insufficient data for {product_type}")
            return None
        
        # Remove rows with missing target values
        valid_idx = ~product_data[target_var].isna()
        product_data = product_data[valid_idx]
        
        if len(product_data) < 30:
            print(f"Insufficient valid data for {product_type}")
            return None
        
        X = product_data[feature_cols]
        y = product_data[target_var]
        
        # Remove features with too many missing values or zero variance
        valid_features = []
        for col in feature_cols:
            if X[col].isna().sum() / len(X) < 0.5 and X[col].std() > 0:
                valid_features.append(col)
        
        X = X[valid_features]
        
        if X.shape[1] == 0:
            print(f"No valid features for {product_type}")
            return None
        
        # Time series split
        n_samples = len(X)
        train_size = int(0.8 * n_samples)
        
        X_train = X.iloc[:train_size]
        X_test = X.iloc[train_size:]
        y_train = y.iloc[:train_size]
        y_test = y.iloc[train_size:]
        
        if len(X_test) < 10:
            print(f"Insufficient test data for {product_type}")
            return None
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=valid_features, index=X_train.index)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=valid_features, index=X_test.index)
        
        # Train models
        models, predictions, scores = self.train_models(X_train_scaled, X_test_scaled, y_train, y_test)
        
        # Calculate feature importance
        importance_results = self.calculate_feature_importance(
            models, X_train_scaled, X_test_scaled, y_test, valid_features
        )
        
        return {
            'models': models,
            'predictions': predictions,
            'scores': scores,
            'importance': importance_results,
            'feature_names': valid_features,
            'test_dates': product_data.iloc[train_size:][self.date_col]
        }
    
    def run_analysis(self):
        """Run complete analysis for all products and targets"""
        print("Running comprehensive OEE analysis...")
        
        # Detect shifts first
        self.detect_shifts()
        
        # Analyze each product-target combination
        for product in self.df[self.product_col].unique():
            if pd.isna(product):
                continue
                
            self.results[product] = {}
            
            for target in self.target_vars:
                if target in self.df.columns:
                    result = self.analyze_product(product, target)
                    if result:
                        self.results[product][target] = result
        
        return self.results
    
    def get_top_features(self, product, target, method='permutation', model='XGBoost', top_n=10):
        """Get top important features for specific product-target combination"""
        if product not in self.results or target not in self.results[product]:
            return None
        
        importance_dict = self.results[product][target]['importance'][model][method]
        
        # Sort by importance (descending)
        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_features[:top_n]
    
    def generate_summary_report(self):
        """Generate comprehensive summary report"""
        print("\n" + "="*80)
        print("OEE ROOT CAUSE ANALYSIS - SUMMARY REPORT")
        print("="*80)
        
        # Shifts summary
        print("\n1. DETECTED DOWNWARD SHIFTS BY PRODUCT:")
        print("-" * 50)
        for product, targets in self.shifts.items():
            print(f"\nProduct: {product}")
            for target, shifts in targets.items():
                if shifts:
                    print(f"  {target}: {len(shifts)} significant downward shifts detected")
                    max_shift = max(shifts, key=lambda x: x['magnitude']) if shifts else None
                    if max_shift:
                        print(f"    Largest shift: {max_shift['magnitude']:.3f} on {max_shift['date'].strftime('%Y-%m-%d')}")
        
        # Model performance summary
        print("\n2. MODEL PERFORMANCE SUMMARY:")
        print("-" * 50)
        for product, targets in self.results.items():
            print(f"\nProduct: {product}")
            for target, result in targets.items():
                print(f"  {target}:")
                for model_name, scores in result['scores'].items():
                    print(f"    {model_name}: R² = {scores['R2']:.3f}, RMSE = {scores['RMSE']:.3f}")
        
        # Top features summary
        print("\n3. TOP FEATURES BY PRODUCT (XGBoost Permutation Importance):")
        print("-" * 50)
        for product, targets in self.results.items():
            print(f"\nProduct: {product}")
            for target, result in targets.items():
                print(f"  {target} - Top 5 Features:")
                top_features = self.get_top_features(product, target, 'permutation', 'XGBoost', 5)
                if top_features:
                    for i, (feature, importance) in enumerate(top_features, 1):
                        print(f"    {i}. {feature}: {importance:.4f}")
        
        print("\n" + "="*80)
        print("Analysis completed. Use plot methods for visualizations.")
        print("="*80)
    
    def plot_feature_importance_comparison(self, product, target, top_n=15):
        """Plot feature importance comparison between models"""
        if product not in self.results or target not in self.results[product]:
            print(f"No results for {product} - {target}")
            return
        
        result = self.results[product][target]
        
        # Create subplots
        fig, axes = plt.subplots(2, 2, figsize=(20, 15))
        fig.suptitle(f'Feature Importance Analysis: {product} - {target}', fontsize=16, fontweight='bold')
        
        methods = ['built_in', 'permutation']
        models = ['Random Forest', 'XGBoost']
        
        for i, model in enumerate(models):
            for j, method in enumerate(methods):
                ax = axes[i, j]
                
                if method in result['importance'][model]:
                    importance_dict = result['importance'][model][method]
                    
                    # Sort and get top features
                    sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]
                    
                    if sorted_features:
                        features, importances = zip(*sorted_features)
                        
                        # Create horizontal bar plot
                        y_pos = np.arange(len(features))
                        bars = ax.barh(y_pos, importances, alpha=0.8)
                        
                        # Color bars by importance level
                        max_imp = max(importances)
                        colors = plt.cm.viridis([imp/max_imp for imp in importances])
                        for bar, color in zip(bars, colors):
                            bar.set_color(color)
                        
                        ax.set_yticks(y_pos)
                        ax.set_yticklabels(features, fontsize=10)
                        ax.set_xlabel('Importance Score')
                        ax.set_title(f'{model} - {method.replace("_", " ").title()}')
                        ax.grid(True, alpha=0.3)
                        
                        # Add value labels
                        for i, bar in enumerate(bars):
                            width = bar.get_width()
                            ax.text(width, bar.get_y() + bar.get_height()/2, 
                                   f'{width:.3f}', ha='left', va='center', fontweight='bold')
        
        plt.tight_layout()
        plt.show()
    
    def plot_shifts_timeline(self, product, target):
        """Plot timeline showing detected shifts"""
        if product not in self.shifts or target not in self.shifts[product]:
            print(f"No shifts detected for {product} - {target}")
            return
        
        shifts = self.shifts[product][target]
        if not shifts:
            print(f"No shifts detected for {product} - {target}")
            return
        
        # Get product data
        product_data = self.df[self.df[self.product_col] == product].copy()
        
        fig, ax = plt.subplots(figsize=(15, 8))
        
        # Plot target variable
        ax.plot(product_data[self.date_col], product_data[target], 
                linewidth=2, alpha=0.7, color='blue', label=f'{target}')
        
        # Mark shifts
        shift_dates = [shift['date'] for shift in shifts]
        shift_magnitudes = [shift['magnitude'] for shift in shifts]
        
        # Get y-values for shift dates
        shift_y_values = []
        for date in shift_dates:
            closest_idx = (product_data[self.date_col] - date).abs().idxmin()
            shift_y_values.append(product_data.loc[closest_idx, target])
        
        ax.scatter(shift_dates, shift_y_values, color='red', s=100, 
                  zorder=5, label='Downward Shifts', marker='v')
        
        # Add annotations for significant shifts
        for i, (date, magnitude, y_val) in enumerate(zip(shift_dates, shift_magnitudes, shift_y_values)):
            ax.annotate(f'Shift: {magnitude:.2f}', 
                       xy=(date, y_val), xytext=(10, 10),
                       textcoords='offset points', fontsize=8,
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                       arrowprops=dict(arrowstyle='->', color='red'))
        
        ax.set_xlabel('Date')
        ax.set_ylabel(f'{target} Value')
        ax.set_title(f'Timeline Analysis: {product} - {target}\nDownward Shifts Detection')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Rotate x-axis labels
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

# Example usage and main analysis function
def main_analysis(df):
    """
    Main analysis function to run complete OEE analysis
    
    Parameters:
    df: pandas DataFrame with the required columns
    
    Returns:
    OEEAnalyzer object with complete analysis results
    """
    
    # Initialize analyzer
    analyzer = OEEAnalyzer(df=df)
    
    # Preprocess data
    analyzer.preprocess_data()
    
    # Run complete analysis
    results = analyzer.run_analysis()
    
    # Generate summary report
    analyzer.generate_summary_report()
    
    return analyzer

# Usage example:
"""
# Load your data
df = pd.read_csv('your_data.csv')

# Run analysis
analyzer = main_analysis(df)

# View specific results
print("\\nTop features for Product A, Y1:")
top_features = analyzer.get_top_features('Product_A', 'Y1', 'permutation', 'XGBoost', 10)
for feature, importance in top_features:
    print(f"{feature}: {importance:.4f}")

# Plot feature importance comparison
analyzer.plot_feature_importance_comparison('Product_A', 'Y1', top_n=10)

# Plot shifts timeline
analyzer.plot_shifts_timeline('Product_A', 'Y1')

# Access raw results
results = analyzer.results
shifts = analyzer.shifts
"""
